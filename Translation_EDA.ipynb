{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation Exploration & Data Cleaning\n",
    "Author Brian Tam, 11/02/2020\n",
    "\n",
    "This notebook is used to clean the [Bible corpus](https://www.kaggle.com/oswinrh/bible) as an intermediate setup to prep it for moding.\n",
    "Specifically this initial process explored the different translations and their individual advantages:\n",
    "1. Total vocabulary (for the purposes of dimensionality reduction)\n",
    "2. How true to the original Greek/Hebrew is the translation\n",
    "For a detailed breakdown look [here](https://commonwaychurch.com/wp-content/uploads/2015/11/bibletranslationchart.pdf)\n",
    "\n",
    "Utlimately I decided to use the BBE translation for its inhertly smaller vocabulary that leads to natural dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:07:18.431539Z",
     "iopub.status.busy": "2020-11-05T00:07:18.427590Z",
     "iopub.status.idle": "2020-11-05T00:07:19.767633Z",
     "shell.execute_reply": "2020-11-05T00:07:19.766996Z",
     "shell.execute_reply.started": "2020-11-05T00:07:18.431029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Get pandas and postgres to work together\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# Panda overides for visuals\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "\n",
    "# Text Preprocessing\n",
    "import re\n",
    "import string\n",
    "# Import spacy to do NLP\n",
    "import spacy\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Import sklearn to do CountVectorizing and TF-IDF document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import matplotlib as plt\n",
    "\n",
    "# Import custom spaCy preprocessing\n",
    "from utilities.text_cleaning import spacy_tokenizer\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# KJV translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the oldest and most well respected versions of the bible\n",
    "- Written in old English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:15:43.122572Z",
     "iopub.status.busy": "2020-11-04T03:15:43.122143Z",
     "iopub.status.idle": "2020-11-04T03:15:43.203562Z",
     "shell.execute_reply": "2020-11-04T03:15:43.202807Z",
     "shell.execute_reply.started": "2020-11-04T03:15:43.122530Z"
    }
   },
   "outputs": [],
   "source": [
    "kjv=pd.read_csv('bible_corpus/bible_databases-master/t_kjv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:15:44.687824Z",
     "iopub.status.busy": "2020-11-04T03:15:44.687515Z",
     "iopub.status.idle": "2020-11-04T03:19:52.737798Z",
     "shell.execute_reply": "2020-11-04T03:19:52.737234Z",
     "shell.execute_reply.started": "2020-11-04T03:15:44.687786Z"
    }
   },
   "outputs": [],
   "source": [
    "kjv['cleaned']=kjv['field.4'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:19:52.739159Z",
     "iopub.status.busy": "2020-11-04T03:19:52.738987Z",
     "iopub.status.idle": "2020-11-04T03:19:52.741909Z",
     "shell.execute_reply": "2020-11-04T03:19:52.741346Z",
     "shell.execute_reply.started": "2020-11-04T03:19:52.739139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into X and y data sets\n",
    "X = kjv.cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:19:52.744406Z",
     "iopub.status.busy": "2020-11-04T03:19:52.744136Z",
     "iopub.status.idle": "2020-11-04T03:19:53.860949Z",
     "shell.execute_reply": "2020-11-04T03:19:53.859947Z",
     "shell.execute_reply.started": "2020-11-04T03:19:52.744361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10114"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf1 = TfidfVectorizer()\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X)\n",
    "len(pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:19:53.862630Z",
     "iopub.status.busy": "2020-11-04T03:19:53.862411Z",
     "iopub.status.idle": "2020-11-04T03:19:54.655057Z",
     "shell.execute_reply": "2020-11-04T03:19:54.654278Z",
     "shell.execute_reply.started": "2020-11-04T03:19:53.862607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaronites</th>\n",
       "      <th>abaddon</th>\n",
       "      <th>abagtha</th>\n",
       "      <th>abana</th>\n",
       "      <th>abarim</th>\n",
       "      <th>abase</th>\n",
       "      <th>abate</th>\n",
       "      <th>abba</th>\n",
       "      <th>abda</th>\n",
       "      <th>...</th>\n",
       "      <th>zorathites</th>\n",
       "      <th>zoreah</th>\n",
       "      <th>zorites</th>\n",
       "      <th>zorobabel</th>\n",
       "      <th>zuar</th>\n",
       "      <th>zuph</th>\n",
       "      <th>zur</th>\n",
       "      <th>zuriel</th>\n",
       "      <th>zurishaddai</th>\n",
       "      <th>zuzims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31098</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31099</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31102</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31103 rows × 10114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aaron  aaronites  abaddon  abagtha  abana  abarim  abase  abate  abba  \\\n",
       "0      0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "1      0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "2      0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "3      0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "4      0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "...    ...    ...        ...      ...      ...    ...     ...    ...    ...    \n",
       "31098  0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "31099  0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "31100  0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "31101  0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "31102  0.0    0.0        0.0      0.0      0.0    0.0     0.0    0.0    0.0    \n",
       "\n",
       "       abda  ...  zorathites  zoreah  zorites  zorobabel  zuar  zuph  zur  \\\n",
       "0      0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "1      0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "2      0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "3      0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "4      0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "...    ...   ...  ...         ...     ...      ...        ...   ...   ...   \n",
       "31098  0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "31099  0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "31100  0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "31101  0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "31102  0.0   ...  0.0         0.0     0.0      0.0        0.0   0.0   0.0   \n",
       "\n",
       "       zuriel  zurishaddai  zuzims  \n",
       "0      0.0     0.0          0.0     \n",
       "1      0.0     0.0          0.0     \n",
       "2      0.0     0.0          0.0     \n",
       "3      0.0     0.0          0.0     \n",
       "4      0.0     0.0          0.0     \n",
       "...    ...     ...          ...     \n",
       "31098  0.0     0.0          0.0     \n",
       "31099  0.0     0.0          0.0     \n",
       "31100  0.0     0.0          0.0     \n",
       "31101  0.0     0.0          0.0     \n",
       "31102  0.0     0.0          0.0     \n",
       "\n",
       "[31103 rows x 10114 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:25:59.262630Z",
     "iopub.status.busy": "2020-11-04T03:25:59.262408Z",
     "iopub.status.idle": "2020-11-04T03:25:59.276200Z",
     "shell.execute_reply": "2020-11-04T03:25:59.275113Z",
     "shell.execute_reply.started": "2020-11-04T03:25:59.262607Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NMF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-eaee8794ffaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mTopicModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdoc_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopicModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NMF' is not defined"
     ]
    }
   ],
   "source": [
    "# Acronynms: Latent Semantic Analysis (LSA) is just another name for \n",
    "#  Signular Value Decomposition (SVD) applied to Natural Language Processing (NLP)\n",
    "\n",
    "TopicModel = NMF(10)\n",
    "doc_topic = TopicModel.fit_transform(pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = display_topics(TopicModel, tfidf1.get_feature_names(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame(TopicModel.components_.round(3),\n",
    "             index =  topics,\n",
    "             columns = tfidf1.get_feature_names())\n",
    "topic_word.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_topic_array = TopicModel.transform(pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_topics = pd.DataFrame(doc_topic.round(5),\n",
    "             index = X.index,\n",
    "             columns = topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_topics.values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBE translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibe in basic English was a translations done by Professor S. H. Hooke following the standards of \"Basic English\", last revised in 1965\n",
    "This implies a couple of restricitons:\n",
    "- Basic English restricts Vocabulary to 1000 words \n",
    "    - 850 base words\n",
    "    - 100 additional words for poetry\n",
    "    - 50 additional words related to biblical context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:07:28.423960Z",
     "iopub.status.busy": "2020-11-05T00:07:28.423742Z",
     "iopub.status.idle": "2020-11-05T00:07:28.711318Z",
     "shell.execute_reply": "2020-11-05T00:07:28.710766Z",
     "shell.execute_reply.started": "2020-11-05T00:07:28.423937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "850"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the default 850 basic english words \n",
    "basic_english = pd.read_pickle('data/basic_english_list')\n",
    "len(basic_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:07:28.851058Z",
     "iopub.status.busy": "2020-11-05T00:07:28.850840Z",
     "iopub.status.idle": "2020-11-05T00:07:28.966381Z",
     "shell.execute_reply": "2020-11-05T00:07:28.965722Z",
     "shell.execute_reply.started": "2020-11-05T00:07:28.851035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import BBE translation to df\n",
    "bbe = pd.read_csv('bible_corpus/bible_databases-master/t_bbe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:07:29.383803Z",
     "iopub.status.busy": "2020-11-05T00:07:29.383582Z",
     "iopub.status.idle": "2020-11-05T00:11:45.601914Z",
     "shell.execute_reply": "2020-11-05T00:11:45.601376Z",
     "shell.execute_reply.started": "2020-11-05T00:07:29.383781Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove lemmetize, remove stop-words and punctuation\n",
    "bbe['cleaned']=bbe['field.4'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:11:45.603557Z",
     "iopub.status.busy": "2020-11-05T00:11:45.603249Z",
     "iopub.status.idle": "2020-11-05T00:11:45.798281Z",
     "shell.execute_reply": "2020-11-05T00:11:45.797699Z",
     "shell.execute_reply.started": "2020-11-05T00:11:45.603525Z"
    }
   },
   "outputs": [],
   "source": [
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "bbe['cleaner']= bbe.cleaned.map(punc_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:11:45.848791Z",
     "iopub.status.busy": "2020-11-05T00:11:45.848541Z",
     "iopub.status.idle": "2020-11-05T00:11:45.893286Z",
     "shell.execute_reply": "2020-11-05T00:11:45.892556Z",
     "shell.execute_reply.started": "2020-11-05T00:11:45.848767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field</th>\n",
       "      <th>field.1</th>\n",
       "      <th>field.2</th>\n",
       "      <th>field.3</th>\n",
       "      <th>field.4</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>cleaner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16102</th>\n",
       "      <td>19123004</td>\n",
       "      <td>19</td>\n",
       "      <td>123</td>\n",
       "      <td>4</td>\n",
       "      <td>For long enough have men of pride made sport of our soul.</td>\n",
       "      <td>long man pride sport soul</td>\n",
       "      <td>long man pride sport soul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16103</th>\n",
       "      <td>19124001</td>\n",
       "      <td>19</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>&amp;lt;A Song of the going up. Of David.&amp;gt; If it had not been the Lord who was on our side (let Israel now say);</td>\n",
       "      <td>lt;a song david.&amp;gt lord let israel</td>\n",
       "      <td>lt a song david  gt lord let israel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16104</th>\n",
       "      <td>19124002</td>\n",
       "      <td>19</td>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>If it had not been the Lord who was on our side, when men came up against us;</td>\n",
       "      <td>lord man come</td>\n",
       "      <td>lord man come</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16105</th>\n",
       "      <td>19124003</td>\n",
       "      <td>19</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>They would have made a meal of us while still living, in the heat of their wrath against us:</td>\n",
       "      <td>meal live heat wrath</td>\n",
       "      <td>meal live heat wrath</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          field  field.1  field.2  field.3  \\\n",
       "16102  19123004  19       123      4         \n",
       "16103  19124001  19       124      1         \n",
       "16104  19124002  19       124      2         \n",
       "16105  19124003  19       124      3         \n",
       "\n",
       "                                                                                                               field.4  \\\n",
       "16102  For long enough have men of pride made sport of our soul.                                                         \n",
       "16103  &lt;A Song of the going up. Of David.&gt; If it had not been the Lord who was on our side (let Israel now say);   \n",
       "16104  If it had not been the Lord who was on our side, when men came up against us;                                     \n",
       "16105  They would have made a meal of us while still living, in the heat of their wrath against us:                      \n",
       "\n",
       "                                   cleaned  \\\n",
       "16102  long man pride sport soul             \n",
       "16103  lt;a song david.&gt lord let israel   \n",
       "16104  lord man come                         \n",
       "16105  meal live heat wrath                  \n",
       "\n",
       "                                   cleaner  \n",
       "16102  long man pride sport soul            \n",
       "16103  lt a song david  gt lord let israel  \n",
       "16104  lord man come                        \n",
       "16105  meal live heat wrath                 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbe.iloc[16102:16106]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T00:11:45.894442Z",
     "iopub.status.busy": "2020-11-05T00:11:45.894260Z"
    }
   },
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://briantam:localhost@localhost/bible')\n",
    "\n",
    "bbe.to_sql('bbe_alchemy', engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T00:51:28.605265Z",
     "iopub.status.busy": "2020-11-04T00:51:28.604997Z",
     "iopub.status.idle": "2020-11-04T00:51:28.871609Z",
     "shell.execute_reply": "2020-11-04T00:51:28.870614Z",
     "shell.execute_reply.started": "2020-11-04T00:51:28.605235Z"
    }
   },
   "outputs": [],
   "source": [
    "bbe.to_csv('data/bbe_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NO** stop_words **YES** basic_english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T01:20:15.316652Z",
     "iopub.status.busy": "2020-11-04T01:20:15.316366Z",
     "iopub.status.idle": "2020-11-04T01:20:16.139626Z",
     "shell.execute_reply": "2020-11-04T01:20:16.139011Z",
     "shell.execute_reply.started": "2020-11-04T01:20:15.316623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  4912\n"
     ]
    }
   ],
   "source": [
    "# Define what you'll feed into the vectorizer as X\n",
    "X = bbe['cleaner']\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "bbe_cleaned_tfidf = tfidf.fit_transform(X)\n",
    "bbe_cleaned_tfidf_df = pd.DataFrame(bbe_cleaned_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "print('Vocab size: ', len(bbe_cleaned_tfidf_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T01:20:49.344466Z",
     "iopub.status.busy": "2020-11-04T01:20:49.344238Z",
     "iopub.status.idle": "2020-11-04T01:20:50.362554Z",
     "shell.execute_reply": "2020-11-04T01:20:50.361660Z",
     "shell.execute_reply.started": "2020-11-04T01:20:49.344442Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f85c4464a61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtolken_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtolken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtolken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmytolkens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBBE_POS_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolken_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtolken_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBBE_POS_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBBE_POS_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vocab Distribution of BBE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "mytolkens = parser(' '.join(list(bbe_cleaned_tfidf_df.columns)))\n",
    "tolken_list = [tolken.pos_ for tolken in mytolkens]\n",
    "BBE_POS_df = pd.DataFrame([(x, tolken_list.count(x)) for x in set(tolken_list)]).sort_values(1)\n",
    "plt.barh(BBE_POS_df[0],BBE_POS_df[1])\n",
    "plt.title('Vocab Distribution of BBE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-03T04:01:37.456509Z",
     "iopub.status.busy": "2020-11-03T04:01:37.456241Z",
     "iopub.status.idle": "2020-11-03T04:01:37.458906Z",
     "shell.execute_reply": "2020-11-03T04:01:37.458338Z",
     "shell.execute_reply.started": "2020-11-03T04:01:37.456484Z"
    }
   },
   "source": [
    "# Trying other stop_word filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NO** stop_words **NO** basic_english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what you'll feed into the vectorizer as X\n",
    "X = bbe['cleaned']\n",
    "tfidf = TfidfVectorizer(stop_words = basic_english)\n",
    "bbe_cleaned_tfidf = tfidf.fit_transform(X)\n",
    "bbe_cleaned_tfidf = pd.DataFrame(bbe_cleaned_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "print('Vocab size: ', len(bbe_cleaned_tfidf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytolkens = parser(' '.join(list(bbe_cleaned_tfidf.columns)))\n",
    "tolken_list = [tolken.pos_ for tolken in mytolkens]\n",
    "BBE_POS_df = pd.DataFrame([(x, tolken_list.count(x)) for x in set(tolken_list)]).sort_values(1)\n",
    "plt.barh(BBE_POS_df[0],BBE_POS_df[1])\n",
    "plt.title('Vocab Distribution of BBE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YES** stop_words **NO** basic_english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what you'll feed into the vectorizer as X\n",
    "X = bbe['cleaned']\n",
    "tfidf = TfidfVectorizer(max_df=.9 stop_words = basic_english)\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X)\n",
    "\n",
    "len(pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names()).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_tfidf = pd.DataFrame(X_train_tfidf1.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytolkens = parser(' '.join(list(bbe_tfidf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolken_list = [tolken.pos_ for tolken in mytolkens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BBE_POS_df = pd.DataFrame([(x, tolken_list.count(x)) for x in set(tolken_list)]).sort_values(1)\n",
    "BBE_POS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(BBE_POS_df[0],BBE_POS_df[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YES** stop_words **YES** basic_english "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what you'll feed into the vectorizer as X\n",
    "X = bbe['field.4']\n",
    "tfidf = TfidfVectorizer()\n",
    "bbe_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "print('Vocab Size: ', len(pd.DataFrame(bbe_tfidf.toarray(), columns=tfidf.get_feature_names()).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbe_tfidf = pd.DataFrame(bbe_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "mytolkens = parser(' '.join(list(bbe_tfidf.columns)))\n",
    "tolken_list = [tolken.pos_ for tolken in mytolkens]\n",
    "BBE_POS_df = pd.DataFrame([(x, tolken_list.count(x)) for x in set(tolken_list)]).sort_values(1)\n",
    "plt.barh(BBE_POS_df[0],BBE_POS_df[1])\n",
    "plt.title('Vocab Distribution of BBE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "text = ' '.join([tolken.pos_ for tolken in mytolkens])\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width = 1000, height = 1000,\n",
    "                background_color =\"rgba(255, 255, 255, 0)\", mode=\"RGBA\").generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(f'POS_BBE_not_in_BE.png',bbox_inches = 'tight', pad_inches = .25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-01T05:44:17.954822Z",
     "iopub.status.busy": "2020-11-01T05:44:17.954592Z",
     "iopub.status.idle": "2020-11-01T05:44:17.957612Z",
     "shell.execute_reply": "2020-11-01T05:44:17.956743Z",
     "shell.execute_reply.started": "2020-11-01T05:44:17.954797Z"
    }
   },
   "source": [
    "### WEB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = pd.read_csv('data/bible_databases-master/t_web.csv')\n",
    "# Assign the \n",
    "X = web['field.4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF of the array of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X)\n",
    "len(pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf.get_feature_names()).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-01T06:01:22.928522Z",
     "iopub.status.busy": "2020-11-01T06:01:22.928328Z",
     "iopub.status.idle": "2020-11-01T06:01:22.930897Z",
     "shell.execute_reply": "2020-11-01T06:01:22.930215Z",
     "shell.execute_reply.started": "2020-11-01T06:01:22.928501Z"
    }
   },
   "source": [
    "### WBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YLT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
